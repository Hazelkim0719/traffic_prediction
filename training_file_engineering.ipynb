{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic file processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import geodesic\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "os.chdir(\"/Users/Tom/Documents/EventsxTraffic/Traffic_Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'signal_id_mercedes_data.csv'\n",
    "output_file = 'temporary_file.csv'\n",
    "\n",
    "with open(input_file, 'r', newline='', encoding='utf-8') as csv_in, \\\n",
    "     open(output_file, 'w', newline='', encoding='utf-8') as csv_out:\n",
    "    reader = csv.reader(csv_in)\n",
    "    writer = csv.writer(csv_out)\n",
    "    \n",
    "    # Iterate over each row in the input CSV\n",
    "    for row in reader:\n",
    "        # Check if the row is not empty (i.e., at least one cell has content)\n",
    "        if any(cell.strip() for cell in row):\n",
    "            writer.writerow(row)\n",
    "\n",
    "os.replace(output_file, input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join long lat coordinates to approach volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = pd.read_csv('signal_id_mercedes_data.csv')\n",
    "atlanta_signals_long_lat_df = pd.read_csv('atlanta_signals_long_lat.csv')\n",
    "\n",
    "# Define a function to merge closest signal coordinates\n",
    "def merge_coordinates(df, signal_column):\n",
    "    return df.merge(\n",
    "        atlanta_signals_long_lat_df, \n",
    "        left_on=signal_column, \n",
    "        right_on=\"SignalID\", \n",
    "        suffixes=('', f'_{signal_column}_Coords')\n",
    "    ).drop(columns=[\"SignalID\"])\n",
    "\n",
    "# Start with the closest_traffic_lights dataset and add coordinates for each closest signal\n",
    "merged_df = input_file.copy()\n",
    "merged_df = merge_coordinates(merged_df, f'Signal_ID')\n",
    "merged_df = merged_df.rename(columns={'Signal_ID': f'SignalID'})\n",
    "\n",
    "merged_df.to_csv(\"signal_id_data_lat_long.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>start_local</th>\n",
       "      <th>end_local</th>\n",
       "      <th>phq_attendance</th>\n",
       "      <th>venue_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Annual America's SBDC Conference</td>\n",
       "      <td>conferences</td>\n",
       "      <td>2024-09-09 09:00:00</td>\n",
       "      <td>2024-09-13 18:00:00</td>\n",
       "      <td>500.0</td>\n",
       "      <td>Atlanta Marriott Marquis</td>\n",
       "      <td>33.761585</td>\n",
       "      <td>-84.385615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IWF - International Woodworking Machinery and ...</td>\n",
       "      <td>expos</td>\n",
       "      <td>2024-08-06 00:00:00</td>\n",
       "      <td>2024-08-09 23:59:59</td>\n",
       "      <td>25524.0</td>\n",
       "      <td>Georgia World Congress Center</td>\n",
       "      <td>33.759006</td>\n",
       "      <td>-84.398739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>International Woodworking Fair Atlanta</td>\n",
       "      <td>expos</td>\n",
       "      <td>2024-08-20 00:00:00</td>\n",
       "      <td>2024-08-23 23:59:59</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>Georgia World Congress Center</td>\n",
       "      <td>33.759006</td>\n",
       "      <td>-84.398739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Annual International Conference</td>\n",
       "      <td>conferences</td>\n",
       "      <td>2024-09-24 09:00:00</td>\n",
       "      <td>2024-09-26 18:00:00</td>\n",
       "      <td>500.0</td>\n",
       "      <td>Hyatt Regency Atlanta</td>\n",
       "      <td>33.761681</td>\n",
       "      <td>-84.387093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Funny Girl</td>\n",
       "      <td>performing-arts</td>\n",
       "      <td>2024-08-01 19:30:00</td>\n",
       "      <td>2024-08-01 19:30:00</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>The Fox Theatre</td>\n",
       "      <td>33.772585</td>\n",
       "      <td>-84.385603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title         category  \\\n",
       "0                   Annual America's SBDC Conference      conferences   \n",
       "1  IWF - International Woodworking Machinery and ...            expos   \n",
       "2             International Woodworking Fair Atlanta            expos   \n",
       "3                    Annual International Conference      conferences   \n",
       "4                                         Funny Girl  performing-arts   \n",
       "\n",
       "           start_local            end_local  phq_attendance  \\\n",
       "0  2024-09-09 09:00:00  2024-09-13 18:00:00           500.0   \n",
       "1  2024-08-06 00:00:00  2024-08-09 23:59:59         25524.0   \n",
       "2  2024-08-20 00:00:00  2024-08-23 23:59:59         20000.0   \n",
       "3  2024-09-24 09:00:00  2024-09-26 18:00:00           500.0   \n",
       "4  2024-08-01 19:30:00  2024-08-01 19:30:00          2950.0   \n",
       "\n",
       "                      venue_name        lat        lon  \n",
       "0       Atlanta Marriott Marquis  33.761585 -84.385615  \n",
       "1  Georgia World Congress Center  33.759006 -84.398739  \n",
       "2  Georgia World Congress Center  33.759006 -84.398739  \n",
       "3          Hyatt Regency Atlanta  33.761681 -84.387093  \n",
       "4                The Fox Theatre  33.772585 -84.385603  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"/Users/Tom/Documents/EventsxTraffic\")\n",
    "\n",
    "event_data = pd.read_csv('Events-Export-Atlanta-from-20240727-to-20240930.csv')\n",
    "\n",
    "event_data = event_data[['title', 'category', 'start_local', 'end_local', 'phq_attendance', 'venue_name', 'lat', 'lon']]\n",
    "\n",
    "event_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Signal_ID_distinct\n",
      "0               7186.0\n",
      "1               7160.0\n",
      "2               8145.0\n",
      "3               7158.0\n",
      "4               7181.0\n",
      "5               8143.0\n",
      "6               7177.0\n",
      "7               7546.0\n",
      "8                 60.0\n",
      "9                 56.0\n",
      "10                92.0\n",
      "11                 8.0\n",
      "12                29.0\n",
      "13                46.0\n",
      "14                 3.0\n",
      "15                11.0\n",
      "16              8142.0\n",
      "17              8148.0\n",
      "18              8136.0\n",
      "19               115.0\n",
      "20               132.0\n",
      "21               165.0\n"
     ]
    }
   ],
   "source": [
    "df = merged_df\n",
    "\n",
    "# Get distinct values in 'column1'\n",
    "distinct_values = df['SignalID'].unique()\n",
    "\n",
    "# Convert it to a DataFrame if you want a similar structure as SQL\n",
    "distinct_df = pd.DataFrame(distinct_values, columns=['Signal_ID_distinct'])\n",
    "\n",
    "print(distinct_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed. Prepared data saved to 'traffic_data_prepared.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/658z_90d41z8_024w56tsnqh0000gq/T/ipykernel_39901/2823516375.py:207: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.         0.         0.         ... 0.99223203 0.99223203 0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  traffic_data.update(event_features)\n",
      "/var/folders/_k/658z_90d41z8_024w56tsnqh0000gq/T/ipykernel_39901/2823516375.py:207: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.         0.         0.         ... 0.99223203 0.99223203 0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  traffic_data.update(event_features)\n",
      "/var/folders/_k/658z_90d41z8_024w56tsnqh0000gq/T/ipykernel_39901/2823516375.py:207: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.  0.  0.  ... 1.5 1.5 0. ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  traffic_data.update(event_features)\n",
      "/var/folders/_k/658z_90d41z8_024w56tsnqh0000gq/T/ipykernel_39901/2823516375.py:207: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.   0.   0.   ... 1.25 1.5  0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  traffic_data.update(event_features)\n",
      "/var/folders/_k/658z_90d41z8_024w56tsnqh0000gq/T/ipykernel_39901/2823516375.py:207: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.   0.   0.   ... 0.25 0.   0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  traffic_data.update(event_features)\n",
      "/var/folders/_k/658z_90d41z8_024w56tsnqh0000gq/T/ipykernel_39901/2823516375.py:211: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  traffic_data['avg_distance_to_events'].fillna(max_distance, inplace=True)\n",
      "/var/folders/_k/658z_90d41z8_024w56tsnqh0000gq/T/ipykernel_39901/2823516375.py:212: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  traffic_data['min_distance_to_event'].fillna(max_distance, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# traffic_feature_engineering.py\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Load and Preprocess Event Data\n",
    "# -------------------------------\n",
    "\n",
    "# Load the event data\n",
    "event_data = pd.read_csv('Events-Export-Atlanta-from-20240727-to-20240930_copy.csv')\n",
    "\n",
    "# Keep only the required columns\n",
    "event_data = event_data[['title', 'category', 'start_local', 'end_local', 'phq_attendance', 'venue_name', 'lat', 'lon']]\n",
    "\n",
    "# Convert time columns to datetime\n",
    "event_data['start_time'] = pd.to_datetime(event_data['start_local'])\n",
    "event_data['end_time'] = pd.to_datetime(event_data['end_local'])\n",
    "\n",
    "# Drop the original start_local and end_local columns\n",
    "event_data.drop(['start_local', 'end_local'], axis=1, inplace=True)\n",
    "\n",
    "# Rename columns for consistency\n",
    "event_data.rename(columns={\n",
    "    'title': 'event_name',\n",
    "    'category': 'event_category',\n",
    "    'phq_attendance': 'event_weight',  # Assuming 'phq_attendance' is used as event weight\n",
    "    'lat': 'event_latitude',\n",
    "    'lon': 'event_longitude'\n",
    "}, inplace=True)\n",
    "\n",
    "# Handle missing values\n",
    "event_data['event_weight'] = pd.to_numeric(event_data['event_weight'], errors='coerce').fillna(0)\n",
    "event_data.dropna(subset=['event_latitude', 'event_longitude'], inplace=True)\n",
    "\n",
    "# Compute event duration in hours\n",
    "event_data['event_duration'] = (event_data['end_time'] - event_data['start_time']).dt.total_seconds() / 3600.0\n",
    "\n",
    "# Optionally encode event categories using one-hot encoding\n",
    "event_category_dummies = pd.get_dummies(event_data['event_category'], prefix='event_category')\n",
    "event_data = pd.concat([event_data, event_category_dummies], axis=1)\n",
    "event_data.drop('event_category', axis=1, inplace=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Load and Preprocess Traffic Signal Data\n",
    "# -------------------------------\n",
    "\n",
    "# Load the traffic data with the new schema\n",
    "traffic_data = pd.read_csv('signal_id_mercedes_data.csv')\n",
    "\n",
    "# Rename columns to match previous naming for consistency\n",
    "traffic_data.rename(columns={\n",
    "    'Signal_ID': 'signal_id',\n",
    "    'StartDate': 'start_time',\n",
    "    'EndDate': 'end_time',\n",
    "    'WestboundVolume': 'volume_west',\n",
    "    'EastboundVolume': 'volume_east',\n",
    "    'NorthboundVolume': 'volume_north',\n",
    "    'SouthboundVolume': 'volume_south'\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert 'start_time' and 'end_time' to datetime with the corrected format\n",
    "traffic_data['start_time'] = pd.to_datetime(traffic_data['start_time'], format='%m/%d/%y %H:%M')\n",
    "traffic_data['end_time'] = pd.to_datetime(traffic_data['end_time'], format='%m/%d/%y %H:%M')\n",
    "\n",
    "# Use 'start_time' as the 'timestamp' for consistency\n",
    "traffic_data['timestamp'] = traffic_data['start_time']\n",
    "\n",
    "# Drop 'start_time' and 'end_time' if not needed\n",
    "# traffic_data.drop(['start_time', 'end_time'], axis=1, inplace=True)\n",
    "\n",
    "# Load signal locations (if available)\n",
    "signal_locations = pd.read_csv('atlanta_signals_long_lat.csv')\n",
    "\n",
    "# Merge traffic data with signal locations\n",
    "traffic_data = traffic_data.merge(signal_locations, on='signal_id', how='left')\n",
    "\n",
    "# Ensure latitude and longitude are numeric\n",
    "traffic_data['latitude'] = pd.to_numeric(traffic_data['latitude'], errors='coerce')\n",
    "traffic_data['longitude'] = pd.to_numeric(traffic_data['longitude'], errors='coerce')\n",
    "\n",
    "# Handle missing values in location data\n",
    "traffic_data.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# Compute total traffic volume\n",
    "traffic_data['total_volume'] = traffic_data[['volume_north', 'volume_south', 'volume_east', 'volume_west']].sum(axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Feature Engineering\n",
    "# -------------------------------\n",
    "\n",
    "# Sort traffic data by signal_id and timestamp\n",
    "traffic_data.sort_values(by=['signal_id', 'timestamp'], inplace=True)\n",
    "\n",
    "# Reset index after sorting\n",
    "traffic_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create lag features for previous total volumes\n",
    "for lag in range(1, 4):\n",
    "    traffic_data[f'total_volume_lag_{lag}'] = traffic_data.groupby('signal_id')['total_volume'].shift(lag)\n",
    "\n",
    "# Fill missing values in lag features with zeros\n",
    "lag_features = [f'total_volume_lag_{lag}' for lag in range(1, 4)]\n",
    "traffic_data[lag_features] = traffic_data[lag_features].fillna(0)\n",
    "\n",
    "# Extract time-based features\n",
    "traffic_data['hour'] = traffic_data['timestamp'].dt.hour\n",
    "traffic_data['day_of_week'] = traffic_data['timestamp'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "traffic_data['is_weekend'] = traffic_data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Encode 'signal_id'\n",
    "le_signal_id = LabelEncoder()\n",
    "traffic_data['signal_id_encoded'] = le_signal_id.fit_transform(traffic_data['signal_id'])\n",
    "\n",
    "# Initialize columns for event features\n",
    "event_feature_cols = [\n",
    "    'num_events',\n",
    "    'total_event_weight',\n",
    "    'avg_distance_to_events',\n",
    "    'min_distance_to_event',\n",
    "    'avg_event_duration',\n",
    "    'avg_time_since_event_start',\n",
    "    'avg_time_until_event_end'\n",
    "]\n",
    "\n",
    "# Include event category dummy columns if using one-hot encoding\n",
    "event_category_cols = event_category_dummies.columns.tolist()\n",
    "event_feature_cols.extend(event_category_cols)\n",
    "\n",
    "for col in event_feature_cols:\n",
    "    traffic_data[col] = 0\n",
    "\n",
    "# Function to compute event features for each traffic record\n",
    "def get_nearby_event_features(row):\n",
    "    timestamp = row['timestamp']\n",
    "    signal_coords = (row['latitude'], row['longitude'])\n",
    "    \n",
    "    # Find active events at the timestamp\n",
    "    active_events = event_data[\n",
    "        (event_data['start_time'] <= timestamp) & (event_data['end_time'] >= timestamp)\n",
    "    ].copy()\n",
    "    \n",
    "    if active_events.empty:\n",
    "        # No active events\n",
    "        return pd.Series({\n",
    "            'num_events': 0,\n",
    "            'total_event_weight': 0,\n",
    "            'avg_distance_to_events': np.nan,\n",
    "            'min_distance_to_event': np.nan,\n",
    "            'avg_event_duration': 0,\n",
    "            'avg_time_since_event_start': 0,\n",
    "            'avg_time_until_event_end': 0,\n",
    "            **{col: 0 for col in event_category_cols}\n",
    "        })\n",
    "    else:\n",
    "        # Calculate distances to events\n",
    "        active_events['distance'] = active_events.apply(\n",
    "            lambda x: geodesic(signal_coords, (x['event_latitude'], x['event_longitude'])).kilometers, axis=1)\n",
    "        \n",
    "        # Filter events within a certain radius (e.g., 1 km)\n",
    "        nearby_events = active_events[active_events['distance'] <= 1.0]\n",
    "        \n",
    "        if nearby_events.empty:\n",
    "            # No nearby events\n",
    "            return pd.Series({\n",
    "                'num_events': 0,\n",
    "                'total_event_weight': 0,\n",
    "                'avg_distance_to_events': np.nan,\n",
    "                'min_distance_to_event': np.nan,\n",
    "                'avg_event_duration': 0,\n",
    "                'avg_time_since_event_start': 0,\n",
    "                'avg_time_until_event_end': 0,\n",
    "                **{col: 0 for col in event_category_cols}\n",
    "            })\n",
    "        else:\n",
    "            # Compute aggregate features\n",
    "            num_events = len(nearby_events)\n",
    "            total_event_weight = nearby_events['event_weight'].sum()\n",
    "            avg_distance = nearby_events['distance'].mean()\n",
    "            min_distance = nearby_events['distance'].min()\n",
    "            avg_event_duration = nearby_events['event_duration'].mean()\n",
    "            nearby_events['time_since_event_start'] = (timestamp - nearby_events['start_time']).dt.total_seconds() / 3600.0\n",
    "            nearby_events['time_until_event_end'] = (nearby_events['end_time'] - timestamp).dt.total_seconds() / 3600.0\n",
    "            avg_time_since_start = nearby_events['time_since_event_start'].mean()\n",
    "            avg_time_until_end = nearby_events['time_until_event_end'].mean()\n",
    "            \n",
    "            # Aggregate event category features\n",
    "            category_sums = nearby_events[event_category_cols].sum()\n",
    "            category_features = category_sums.to_dict()\n",
    "            \n",
    "            # Combine all features into a single series\n",
    "            features = {\n",
    "                'num_events': num_events,\n",
    "                'total_event_weight': total_event_weight,\n",
    "                'avg_distance_to_events': avg_distance,\n",
    "                'min_distance_to_event': min_distance,\n",
    "                'avg_event_duration': avg_event_duration,\n",
    "                'avg_time_since_event_start': avg_time_since_start,\n",
    "                'avg_time_until_event_end': avg_time_until_end\n",
    "            }\n",
    "            # Add category features\n",
    "            features.update(category_features)\n",
    "            \n",
    "            return pd.Series(features)\n",
    "\n",
    "# Apply the function to the traffic data\n",
    "event_features = traffic_data.apply(get_nearby_event_features, axis=1)\n",
    "\n",
    "# Merge the event features back into traffic_data\n",
    "traffic_data.update(event_features)\n",
    "\n",
    "# Handle missing values in distance features\n",
    "max_distance = traffic_data[['avg_distance_to_events', 'min_distance_to_event']].max().max()\n",
    "traffic_data['avg_distance_to_events'].fillna(max_distance, inplace=True)\n",
    "traffic_data['min_distance_to_event'].fillna(max_distance, inplace=True)\n",
    "\n",
    "# Replace any remaining NaNs with zeros\n",
    "traffic_data.fillna(0, inplace=True)\n",
    "\n",
    "# Create interaction features\n",
    "traffic_data['num_events_hour'] = traffic_data['num_events'] * traffic_data['hour']\n",
    "traffic_data['total_event_weight_day_of_week'] = traffic_data['total_event_weight'] * traffic_data['day_of_week']\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Finalize Feature Set\n",
    "# -------------------------------\n",
    "\n",
    "# Define the final list of feature columns\n",
    "feature_cols = [\n",
    "    'num_events',\n",
    "    'total_event_weight',\n",
    "    'avg_distance_to_events',\n",
    "    'min_distance_to_event',\n",
    "    'avg_event_duration',\n",
    "    'avg_time_since_event_start',\n",
    "    'avg_time_until_event_end',\n",
    "    'hour',\n",
    "    'day_of_week',\n",
    "    'is_weekend',\n",
    "    'signal_id_encoded',\n",
    "    # Lag features\n",
    "    'total_volume_lag_1',\n",
    "    'total_volume_lag_2',\n",
    "    'total_volume_lag_3',\n",
    "    # Interaction features\n",
    "    'num_events_hour',\n",
    "    'total_event_weight_day_of_week'\n",
    "] + event_category_cols  # Include event category features if used\n",
    "\n",
    "# Select the final dataset for modeling\n",
    "model_data = traffic_data[feature_cols + ['total_volume']]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Save the Prepared Data\n",
    "# -------------------------------\n",
    "\n",
    "# Save the prepared data to a CSV file\n",
    "model_data.to_csv('traffic_data_prepared.csv', index=False)\n",
    "\n",
    "# -------------------------------\n",
    "# Script Completed\n",
    "# -------------------------------\n",
    "\n",
    "print(\"Feature engineering completed. Prepared data saved to 'traffic_data_prepared.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eventmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
